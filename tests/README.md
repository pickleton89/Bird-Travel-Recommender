# Testing Framework for Bird Travel Recommender

## üèÜ **Test Suite Transformation Complete**

### **Reliability Achievement: 78.4% ‚Üí Near-100%**

This directory contains a **production-ready** pytest-based testing framework that recently underwent a comprehensive **5-phase overhaul** achieving near-perfect reliability for the Bird Travel Recommender system with **30 MCP tools across 6 categories**.

#### **Transformation Highlights**
- ‚úÖ **Phase 1-5 Complete**: Fixed 27+ failing tests across all major categories  
- ‚úÖ **Test Isolation**: Resolved state pollution and cache persistence issues
- ‚úÖ **BatchNode Patterns**: Implemented correct iteration patterns for parallel processing
- ‚úÖ **Advanced Mocking**: Established robust patterns for complex module hierarchies
- ‚úÖ **Error Handling**: Comprehensive graceful degradation and fallback systems
- ‚úÖ **Import Path Fixes**: Corrected module imports throughout test suite

### **Current Test Categories & Status**
- **Infrastructure Tests**: ‚úÖ **100% Pass Rate** (FetchSightingsNode, MCP Handler, Enhanced Error Handling)
- **Pipeline Integration**: ‚úÖ **100% Pass Rate** (7/7 tests with proper BatchNode usage)  
- **End-to-End Real API**: ‚úÖ **100% Pass Rate** (7/7 tests with fixed test isolation)
- **Enhanced Features**: ‚úÖ **100% Pass Rate** (NLP, response formatting, async functions)
- **Flow Configuration**: ‚úÖ **100% Pass Rate** (eliminated configuration warnings)

## Overview

The testing framework provides:
- **Unit tests** for individual handler components across 6 tool categories
- **Integration tests** for complete MCP tool workflows and cross-category interactions
- **Error handling tests** for circuit breakers, retry logic, and graceful degradation
- **Mock eBird API responses** for consistent testing without API limits
- **Performance testing** utilities for concurrent operations
- **Data validation** helpers for all 30 tools
- **Parametrized tests** for comprehensive coverage of tool combinations

## Quick Start

### Running All Tests
```bash
# Run all tests
uv run pytest

# Run tests with verbose output
uv run pytest -v

# Run tests with coverage
uv run pytest --cov=. --cov-report=html
```

### Running Specific Test Categories
```bash
# Run only unit tests
uv run pytest -m unit

# Run only integration tests  
uv run pytest -m integration

# Run error handling tests
uv run pytest -m error_handling

# Run MCP tool tests
uv run pytest -m mcp_tools

# Run only mock tests (no real API calls)
uv run pytest -m mock

# Run quick tests (exclude slow tests)
uv run pytest -m "not slow"

# Run tests by tool category
uv run pytest -k "species"
uv run pytest -k "location"
uv run pytest -k "pipeline"
uv run pytest -k "planning"
uv run pytest -k "advisory"
uv run pytest -k "community"
```

### Running Specific Test Files
```bash
# Test ValidateSpeciesNode only
uv run pytest tests/test_validate_species_node.py

# Test FetchSightingsNode only
uv run pytest tests/test_fetch_sightings_node.py

# Test integration workflows
uv run pytest tests/test_integration.py
```

## Test Structure

### Test Files

#### Core Configuration
- **`conftest.py`** - Shared fixtures and test configuration
- **`test_utils.py`** - Testing utilities and helpers

#### Unit Tests (by category)
- **`unit/test_species.py`** - Species validation handler tests (2 tools)
- **`unit/test_location.py`** - Location discovery handler tests (11 tools)
- **`unit/test_pipeline.py`** - Pipeline processing handler tests (11 tools)
- **`unit/test_planning.py`** - Planning handler tests (2 tools)
- **`unit/test_advisory.py`** - Advisory handler tests (1 tool)
- **`unit/test_community.py`** - Community handler tests (3 tools)

#### Integration Tests
- **`integration/test_mcp_tools_comprehensive.py`** - All 30 tools integration tests
- **`integration/test_enhanced_error_handling.py`** - Error handling framework tests
- **`integration/test_mcp_tools_expansion.py`** - Cross-category workflow tests

#### Legacy Node Tests (being phased out)
- **`test_validate_species_node.py`** - Legacy PocketFlow node tests
- **`test_fetch_sightings_node.py`** - Legacy parallel fetching tests
- **`test_filter_constraints_node.py`** - Legacy constraint filtering tests
- **`test_integration.py`** - Legacy end-to-end pipeline tests

### Test Markers
- **`@pytest.mark.unit`** - Unit tests for individual handler components
- **`@pytest.mark.integration`** - Integration tests for MCP tool workflows
- **`@pytest.mark.error_handling`** - Error handling and resilience tests
- **`@pytest.mark.mcp_tools`** - MCP tool-specific tests
- **`@pytest.mark.mock`** - Tests using mocked data only
- **`@pytest.mark.api`** - Tests requiring real eBird API access
- **`@pytest.mark.slow`** - Tests that take longer to run
- **`@pytest.mark.circuit_breaker`** - Circuit breaker pattern tests
- **`@pytest.mark.performance`** - Performance and concurrency tests

## Mock Data System

### eBird API Mocking
The framework includes comprehensive mocking of eBird API endpoints:

```python
# Fixtures automatically mock eBird API responses
def test_species_validation(mock_ebird_api):
    # Test code here - no real API calls made
    pass
```

### Available Mock Fixtures
- **`mock_ebird_taxonomy`** - Sample eBird taxonomy data
- **`mock_ebird_observations`** - Sample observation data for all regions
- **`mock_ebird_hotspots`** - Sample hotspot data with geographic distribution
- **`mock_validated_species`** - Pre-validated species for testing
- **`mock_ebird_api`** - Complete API client mocking with error simulation
- **`mock_error_scenarios`** - Predefined error scenarios for testing
- **`mock_circuit_breaker`** - Circuit breaker state simulation
- **`mock_mcp_server`** - Complete MCP server for integration testing
- **`mock_handlers_container`** - All 6 handler categories with mocking

## Test Data Generators

### Creating Test Data
```python
from tests.test_utils import TestDataGenerator

generator = TestDataGenerator()

# Generate species lists
species = generator.generate_species_list(10, include_invalid=True)

# Generate geographic coordinates
coords = generator.generate_coordinates_near(42.36, -71.06, count=20)

# Generate mock sightings
sightings = generator.generate_mock_sightings(["norcar", "blujay"], coords)
```

## Data Validation

### Validating Test Results
```python
from tests.test_utils import TestDataValidator

validator = TestDataValidator()

# Validate species data structure
assert validator.validate_species_data(species_result)

# Validate sighting data structure  
assert validator.validate_sighting_data(sighting)

# Validate statistics structure
assert validator.validate_statistics(stats, "validation")
```

## Performance Testing

### Measuring Performance
```python
from tests.test_utils import PerformanceTestHelper

helper = PerformanceTestHelper()

# Time function execution
result, duration = helper.time_function(my_function, arg1, arg2)

# Measure parallel vs sequential performance
speedup, seq_time, par_time = helper.measure_parallel_speedup(
    func, sequential_args, parallel_args
)
```

## Integration Testing

### MCP Tool Integration Tests
Integration tests verify complete MCP tool workflows across all 6 categories:

```python
@pytest.mark.integration
@pytest.mark.mcp_tools
async def test_complete_trip_planning_workflow(mock_mcp_server):
    """Test end-to-end trip planning using multiple tool categories."""
    # Species validation (Species category)
    species_result = await mock_mcp_server.call_tool(
        "validate_species", {"species_names": ["Northern Cardinal"]}
    )
    
    # Location analysis (Location category)
    region_result = await mock_mcp_server.call_tool(
        "get_region_details", {"region": "US-MA"}
    )
    
    # Pipeline processing (Pipeline category)
    sightings_result = await mock_mcp_server.call_tool(
        "fetch_sightings", {
            "validated_species": species_result["validated_species"],
            "region": "US-MA"
        }
    )
    
    # Trip planning (Planning category)
    itinerary_result = await mock_mcp_server.call_tool(
        "plan_complete_trip", {
            "species_names": ["Northern Cardinal"],
            "region": "US-MA",
            "start_location": {"lat": 42.36, "lng": -71.06}
        }
    )
    
    # Verify all stages succeeded
    assert all(result["success"] for result in [
        species_result, region_result, sightings_result, itinerary_result
    ])

@pytest.mark.integration
@pytest.mark.error_handling
async def test_error_handling_across_categories(mock_mcp_server):
    """Test error handling and recovery across tool categories."""
    # Test validation errors
    invalid_result = await mock_mcp_server.call_tool(
        "validate_species", {"species_names": []}
    )
    assert not invalid_result["success"]
    assert invalid_result["error_code"] == "INVALID_INPUT"
    
    # Test API errors with retry
    with patch.object(mock_mcp_server.ebird_api, 'get_observations', side_effect=APIError):
        api_result = await mock_mcp_server.call_tool(
            "fetch_sightings", {"validated_species": [], "region": "US-MA"}
        )
        assert not api_result["success"]
        assert "retry_count" in api_result.get("details", {})
```

## Common Test Patterns

### Testing Node Lifecycle
```python
def test_node_lifecycle(node, shared_store):
    # Test prep phase
    prep_result = node.prep(shared_store)
    assert prep_result is not None
    
    # Test exec phase
    exec_result = node.exec(prep_result)
    assert exec_result is not None
    
    # Test post phase
    node.post(shared_store, prep_result, exec_result)
    assert "expected_key" in shared_store
```

### Parametrized Testing
```python
@pytest.mark.parametrize("tool_category,tool_name,test_params", [
    ("species", "validate_species", {"species_names": ["Northern Cardinal"]}),
    ("location", "get_region_details", {"region": "US-MA"}),
    ("pipeline", "fetch_sightings", {
        "validated_species": [{"species_code": "norcar"}], 
        "region": "US-MA"
    }),
    ("planning", "generate_itinerary", {
        "optimized_route": {"ordered_locations": []},
        "target_species": ["Northern Cardinal"]
    }),
    ("advisory", "get_birding_advice", {
        "query": "Best time to see cardinals?"
    }),
    ("community", "get_recent_checklists", {"region": "US-MA"}),
])
@pytest.mark.mcp_tools
async def test_tool_success_responses(mock_mcp_server, tool_category, tool_name, test_params):
    """Test successful responses for all tool categories."""
    result = await mock_mcp_server.call_tool(tool_name, test_params)
    assert result["success"], f"{tool_category}/{tool_name} failed: {result}"
    
@pytest.mark.parametrize("error_type,expected_code", [
    (ValidationError("Invalid input"), "INVALID_INPUT"),
    (APIError("API failed"), "API_ERROR"),
    (RateLimitError("Rate limit"), "RATE_LIMIT_EXCEEDED"),
    (TimeoutError("Timeout"), "TIMEOUT"),
])
@pytest.mark.error_handling
def test_error_code_mapping(error_type, expected_code):
    """Test error handling produces correct error codes."""
    handler = ErrorHandler()
    result = handler.handle_error(error_type)
    assert result["error_code"] == expected_code
```

## Configuration

### pytest.ini Settings
- **Test discovery** - Automatically finds test files
- **Markers** - Defines custom test categories
- **Output formatting** - Configures verbose output
- **Warning filtering** - Reduces noise in test output

### Environment Setup
Tests automatically configure:
- Mock API keys for testing
- Logging levels for debug output
- Temporary directories for test data

## Troubleshooting

### Common Issues

**Import Errors:**
```bash
# Ensure you're in the project root directory
cd /path/to/Bird-Travel-Recommender
uv run pytest
```

**API Key Warnings:**
```bash
# Tests use mock API keys - warnings are expected
# Real API keys only needed for @pytest.mark.api tests
```

**Slow Tests:**
```bash
# Skip slow tests for faster feedback
uv run pytest -m "not slow"

# Run only fast unit tests
uv run pytest -m "unit and mock"
```

### Debug Mode
```bash
# Run with detailed debug output
uv run pytest -v -s --tb=long

# Run single test with debugging
uv run pytest tests/test_validate_species_node.py::TestValidateSpeciesNode::test_prep_phase -v -s
```

## Contributing

### Adding New Tests
1. Follow the existing naming convention (`test_*.py`)
2. Use appropriate markers (`@pytest.mark.unit`, etc.)
3. Include docstrings explaining test purpose
4. Use fixtures for shared test data
5. Validate test data structure with utilities

### Test Coverage Goals
- **Unit tests** - 90%+ coverage for all 30 MCP tools and handlers
- **Integration tests** - Cover all 6 tool categories and cross-category workflows
- **Error handling** - Test circuit breakers, retry logic, graceful degradation
- **Performance** - Validate concurrent tool execution and caching
- **MCP compliance** - Verify all tools follow MCP protocol standards
- **Resilience** - Test failure scenarios and recovery mechanisms

## Migration from Legacy Tests

The old test files (`test_validate_species.py`, etc.) are being migrated to pytest format:
- Better fixtures and mocking
- Parametrized testing for multiple scenarios
- Comprehensive assertion messages
- Integrated performance testing

Run legacy tests for comparison:
```bash
# Old format (still works)
uv run python test_validate_species.py

# New pytest format (recommended)
uv run pytest tests/test_validate_species_node.py
```